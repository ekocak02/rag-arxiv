import logging
import datetime
from pathlib import Path
from typing import List, Dict
import re  # Used for marker extraction

# Import all modules
import db_manager
import data_ingestor
import vector_indexer
import rag_reporter
import local_translator

# Main logging configuration (All modules will inherit this)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(module)s] - %(message)s'
)
# Reduce noise from noisy libraries
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("ollama").setLevel(logging.WARNING)

# === MAIN SETTINGS (STRATEGY: LINKEDIN) ===

# This is the BASE SEARCH query used by the retriever (Step 1 of Map-Reduce)
# to find the 100 most relevant papers.
BASE_RAG_QUERY = """
Innovations, novel techniques, breakthroughs, and significant findings
in Artificial Intelligence, Data Science, and Machine Learning.
"""

# Directory to store the output posts
OUTPUT_DIR = "daily_linkedin_post"
BASE_DIR = Path(__file__).resolve().parent

# We will fetch data daily
DAYS_AGO = 1

# Static Disclaimers for the final post
DISCLAIMER_EN = (
    "*(This post was generated by an AI assistant (Local RAG System) "
    "based on arXiv abstracts. Please verify all claims.)*\n\n---\n\n"
)
DISCLAIMER_TR = (
    "*(Bu gönderi, arXiv özetlerine dayalı olarak bir YZ asistanı (Lokal RAG Sistemi) "
    "tarafından oluşturulmuştur. Lütfen tüm iddiaları doğrulayın.)*\n\n---\n\n"
)

# Markers for RAG output parsing (as defined in rag_reporter.py)
POST_START_MARKER = "<<<POST_START>>>"
POST_END_MARKER = "<<<POST_END>>>"


def extract_post_from_markers(
    raw_llm_output: str, 
    start_marker: str, 
    end_marker: str
) -> str:
    """
    Extracts the content between the START and END markers provided by the LLM.
    
    If markers are not found, it returns the raw (stripped) text and logs
    a warning, allowing the user to review the raw LLM output.
    
    Args:
        raw_llm_output (str): The full text output from the LLM.
        start_marker (str): The start marker (e.g., "<<<POST_START>>>").
        end_marker (str): The end marker (e.g., "<<<POST_END>>>").
        
    Returns:
        str: The extracted (and stripped) content, or the original
             (stripped) text if markers are not found.
    """
    try:
        # Use re.search with DOTALL (s) flag to find content across newlines
        pattern = re.escape(start_marker) + "(.*?)" + re.escape(end_marker)
        match = re.search(pattern, raw_llm_output, re.DOTALL)
        
        if match:
            # Group 1 contains the content *between* the markers
            extracted_content = match.group(1).strip()
            logging.info("✅ Successfully extracted post content from LLM markers.")
            return extracted_content
        else:
            # Fallback: review output if marker extraction fails
            logging.warning(
                f"⚠️ Could not find markers ({start_marker}) in LLM output. "
                "Returning raw text. Review 'rag_reporter.py' prompt template."
            )
            # Return the raw text (stripped) so the user can see the issue
            return raw_llm_output.strip()
            
    except Exception as e:
        # Failsafe in case of regex error
        logging.error(f"❌ Error during marker extraction: {e}")
        return raw_llm_output.strip()


def save_linkedin_posts(
    en_content: str,
    tr_content: str,
    run_date_iso: str
) -> None:
    """
    Saves the final English and Turkish posts, prepending disclaimers
    and appending the date.
    """
    try:
        output_path = BASE_DIR / OUTPUT_DIR
        output_path.mkdir(exist_ok=True) # Create directory if it doesn't exist

        # 1. Prepare file content (Disclaimer + Body + Date)
        en_filename = f"EN_post_{run_date_iso}.md"
        tr_filename = f"TR_post_{run_date_iso}.md"

        en_full_content = (
            f"{DISCLAIMER_EN}"
            f"{en_content}\n\n" # Only the post content
            f"---\n\n"
            f"*Posted on: {run_date_iso}*"
        )
        tr_full_content = (
            f"{DISCLAIMER_TR}"
            f"{tr_content}\n\n" # Only the post content
            f"---\n\n"
            f"*Yayın tarihi: {run_date_iso}*" # Turkish date marker for TR file
        )

        # 2. Save EN file
        en_file_path = output_path / en_filename
        with open(en_file_path, 'w', encoding='utf-8') as f:
            f.write(en_full_content)
        logging.info(f"✅ English post saved: {en_file_path}")

        # 3. Save TR file
        tr_file_path = output_path / tr_filename
        with open(tr_file_path, 'w', encoding='utf-8') as f:
            f.write(tr_full_content)
        logging.info(f"✅ Turkish post saved: {tr_file_path}")

    except Exception as e:
        logging.error(f"❌ Failed to save posts to file: {e}")


# === MAIN WORKFLOW (DAILY LINKEDIN POST) ===

def main_workflow():
    """
    Executes the full workflow for daily LinkedIn post generation.
    """
    logging.info("="*50)
    logging.info("Daily LinkedIn Post System Initialized")
    logging.info(f"STRATEGY: Daily Fetch (DAYS_AGO={DAYS_AGO}), Map-Reduce RAG")
    logging.info("="*50)

    report_run_date = datetime.date.today()
    report_run_date_iso = report_run_date.isoformat()

    # We filter for the date of the data we ingested (yesterday).
    ingested_data_date = report_run_date - datetime.timedelta(days=DAYS_AGO)
    ingested_data_date_iso = ingested_data_date.isoformat()

    # --- Step 1/5: Data Ingestion (Module 2) (FETCH 1 DAY) ---
    logging.info(f"[1/5] Initializing DB and fetching new papers (DAYS_AGO={DAYS_AGO})...")
    db_manager.initialize_database()
    
    new_papers = data_ingestor.fetch_new_papers(
        days_ago=DAYS_AGO,
        use_manual_pagination=True # Recommended for stability
    )
    
    if not new_papers:
        logging.info("✅ No new papers found for today. Process complete.")
        return

    logging.info(f"Found {len(new_papers)} new papers to process.")

    # --- Step 2/5: Indexing (Module 3) ---
    logging.info("[2/5] Loading vector index and indexing new papers...")
    try:
        vector_index = vector_indexer.get_vector_index()
        vector_indexer.index_papers(new_papers, vector_index)
        logging.info("Indexing complete.")
    except Exception as e:
        logging.error(f"❌ Critical error during vector indexing: {e}")
        return # Do not continue if indexing fails

    # --- Step 3/5: RAG Post Generation (Module 4) (MAP-REDUCE) ---
    logging.info("[3/5] Generating LinkedIn post via Map-Reduce RAG...")
    
    date_filter = ingested_data_date_iso
    
    # The 'cited_ids' list is no longer used in this workflow, so we ignore it.
    generated_post_raw, _ = rag_reporter.generate_linkedin_post_map_reduce(
        base_query=BASE_RAG_QUERY,
        date_filter_str=date_filter
    )
    
    if not generated_post_raw:
        logging.error("❌ RAG engine failed to generate a post (or no data found).")
        return
    
    # Extract the post content from between the markers
    generated_post = extract_post_from_markers(
        generated_post_raw,
        POST_START_MARKER,
        POST_END_MARKER
    )
    
    logging.info("✅ RAG post processed.")

    # The final content is just the processed post text.
    final_english_content = generated_post

    # --- Step 4/5: Translation (Module 5) ---
    logging.info("[4/5] Translating post content to Turkish...")
    
    # Translate only the post body
    turkish_content = local_translator.translate_to_turkish(final_english_content)
    
    if "[TRANSLATION FAILED]" in turkish_content:
        logging.error("❌ An error occurred during local translation. Posts not saved.")
        return
        
    logging.info("Translation complete.")

    # --- Step 5/5: Save EN and TR Posts ---
    logging.info("[5/5] Saving final English (EN) and Turkish (TR) posts...")
    
    save_linkedin_posts(
        en_content=final_english_content,
        tr_content=turkish_content,
        run_date_iso=report_run_date_iso
    )
    
    logging.info("="*50)
    logging.info("✅ Daily LinkedIn Post Generation Completed.")
    logging.info(f"Output files saved to '{OUTPUT_DIR}' directory.")
    logging.info("="*50)

# === END OF WORKFLOW ===


if __name__ == "__main__":
    main_workflow()