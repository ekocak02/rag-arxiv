import logging
from pathlib import Path
from datetime import datetime, timedelta # Added timedelta for main block

# LlamaIndex core components
from llama_index.core import (
    VectorStoreIndex,
    Settings
)
# Ollama LLM
from llama_index.llms.ollama import Ollama
# Ollama Embedding Model
from llama_index.embeddings.ollama import OllamaEmbedding
# LanceDB Vector Store
from llama_index.vector_stores.lancedb import LanceDBVectorStore

# Logging configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# === Model and Database Settings (Plan A2 - Module 4) ===

# Please enter the name of your embedding model running in Ollama
# (This MUST be the same as in 'vector_indexer.py')
EMBED_MODEL_NAME = "embeddinggemma:300m" 

# Please enter the name of your LLM (generator) model running in Ollama
# (e.g., "gemma3:4b")
LLM_MODEL_NAME = "gemma3:4b"

# Directory where the local vector store is saved
LANCEDB_DIR = "./lancedb_storage"
LANCEDB_TABLE_NAME = "arxiv_papers"
BASE_DIR = Path(__file__).resolve().parent
LANCEDB_PATH = BASE_DIR / LANCEDB_DIR

# === FEATURE: METADATA FILTERING STARTS HERE ===

def generate_english_report(query: str, date_filter_str: str = None) -> str:
    """
    Queries the cumulative vector database (LanceDB) using a RAG pipeline
    and generates an English report via a local LLM (Ollama).
    
    Updated version: Applies a date metadata filter if provided.
    
    Args:
        query (str): The prompt/question for the LLM.
        date_filter_str (Optional[str]): A date in 'YYYY-MM-DD' format. 
                         If provided, the search is restricted to this day.
        
    Returns:
        str: The English report generated by the LLM, or an empty string on failure.
    """
    
    logging.info(f"RAG reporter initialized...")
    logging.info(f"Using LLM: {LLM_MODEL_NAME}, Embed Model: {EMBED_MODEL_NAME}")
    
    try:
        # 1. Configure Settings
        Settings.llm = Ollama(model=LLM_MODEL_NAME, request_timeout=1000.0)
        Settings.embed_model = OllamaEmbedding(model_name=EMBED_MODEL_NAME)

        # 2. Load the vector store (LanceDB)
        vector_store = LanceDBVectorStore(
            uri=str(LANCEDB_PATH),
            table_name=LANCEDB_TABLE_NAME,
            mode="read_only"  # We will only be reading from the store
        )

        # 3. Load the index from the existing store
        index = VectorStoreIndex.from_vector_store(vector_store=vector_store)
        
        # 4. Build the Query Engine
        
        # top_k (number of papers) can be lower for daily queries
        # and higher for unfiltered weekly queries.
        # Let's use 20 if filtered, 50 otherwise.
        top_k = 20 if date_filter_str else 50
        
        query_engine_kwargs = {"similarity_top_k": top_k}
        
        if date_filter_str:
            # LanceDB supports SQL-like WHERE clauses on metadata.
            # 'vector_indexer.py' saves the date as "YYYY-MM-DD HH:MM:SS".
            # We use 'LIKE' to filter all records starting with that day.
            where_clause = f"metadata.published_date LIKE '{date_filter_str}%'"
            
            # We pass the filter to the query engine via 'vector_store_kwargs'
            query_engine_kwargs["vector_store_kwargs"] = {"where": where_clause}
            
            logging.info(f"Applying metadata filter: {where_clause}")

        # Build the query engine with our dynamic settings
        query_engine = index.as_query_engine(**query_engine_kwargs)
        
        logging.info(f"Query engine ready. Sending query: '{query[:50]}...'")
        
        # 5. Run the query
        response = query_engine.query(query)
        
        logging.info("✅ English report generated successfully.")
        return str(response)
        
    except Exception as e:
        logging.error(f"❌ Error during RAG report generation: {e}")
        return ""

# === FEATURE: METADATA FILTERING ENDS HERE ===


# If this script is run directly, it performs a test report generation.
if __name__ == "__main__":
    logging.info("RAG Reporter Module (rag_reporter.py) testing...")
    
    # Placeholders for the user to fill in
    PLACEHOLDER_LLM = "<LLM_MODEL_ADI>"
    PLACEHOLDER_EMBED = "<EMBEDDING_MODEL_ADI>"
    
    if (LLM_MODEL_NAME == PLACEHOLDER_LLM or 
        EMBED_MODEL_NAME == PLACEHOLDER_EMBED or 
        LLM_MODEL_NAME == "" or EMBED_MODEL_NAME == ""):
        
        logging.error(f"Please update 'LLM_MODEL_NAME' and 'EMBED_MODEL_NAME' in '{__file__}'.")
    else:
        # Example query specified in Plan A2 - Module 4
        TEST_QUERY = """
        Based on the latest documents (research paper abstracts) provided as context, 
        summarize the key trends in AI and Data Science.
        
        Identify 2-3 main themes. For each theme:
        1. Provide a detailed explanation.
        2. Mention relevant paper titles (or arXiv IDs).
        """
        
        # --- TEST 1: Unfiltered (Full Week) ---
        logging.info("\n--- TEST 1: UNFILTERED (FULL WEEK) ---")
        weekly_report = generate_english_report(TEST_QUERY)
        if weekly_report:
            print(weekly_report)
        else:
            print("Report could not be generated.")

        # --- TEST 2: Filtered (Specific Day) ---
        # (This test requires data from that day to exist in 'lancedb_storage')
        test_day = (datetime.now().date() - timedelta(days=3)).isoformat()
        logging.info(f"\n--- TEST 2: FILTERED (DAY: {test_day}) ---")
        
        daily_report = generate_english_report(TEST_QUERY, date_filter_str=test_day)
        
        if daily_report:
            print(daily_report)
        else:
            print("Report could not be generated (or no data found for that day).")